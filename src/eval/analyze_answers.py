#!/usr/bin/env python3
"""
Purpose: Load, merge, and analyze report datasets generated by different sources/models.

Inputs:
    - JSON files of model/report outputs located in experiment answers/<base>_vs_<compare>/

Outputs:
    - Merged DataFrame
    - Agreement metrics printed to console
    - Further analysis per task
    - Results/logs saved into analysis/<base>_vs_<compare>/
"""

import sys
import os
import argparse
import logging
from pathlib import Path

# -----------------------------
# Adjust module path
# -----------------------------
module_dir = "../../"  # relative path to src folder
sys.path.append(os.path.abspath(module_dir))

# -----------------------------
# Import functions
# -----------------------------

from src.eval.evaluation import calculate_agreement, agreement_x_analysis, load_and_merge_reports


def parse_args():
    parser = argparse.ArgumentParser(
        description="Analyze pairwise answers: merge reports and compute agreement metrics."
    )
    parser.add_argument(
        "--experiment",
        type=str,
        required=True,
        help="Experiment folder name under data/experiments/"
    )
    parser.add_argument(
        "--base",
        type=str,
        required=True,
        help="Base report key (e.g., groundtruth)"
    )
    parser.add_argument(
        "--compare",
        type=str,
        required=True,
        help="Compare report key (e.g., discovera(gpt-4o))"
    )
    parser.add_argument(
        "--model",
        type=str,
        default="gpt-5",
        help="Model name (e.g., gpt-5)"
    )
    parser.add_argument(
        "--provider",
        type=str,
        default="openai",
        help="Provider name (default: openai)"
    )
    return parser.parse_args()


def setup_logger(log_dir: Path, base: str, compare: str, model: str):
    log_dir.mkdir(parents=True, exist_ok=True)
    log_file = log_dir / f"analysis_{base}_vs_{compare}_{model}.log"
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(message)s",
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler(log_file, mode="w")
        ]
    )
    logging.info(f"Logging to {log_file}")


def main():
    args = parse_args()

    # -----------------------------
    # Define paths
    # -----------------------------
    base_path = Path("../../data/experiments") / args.experiment
    answers_folder = base_path / "answers" / f"{args.base}_vs_{args.compare}"
    if not answers_folder.exists():
        raise FileNotFoundError(f"Answers folder not found: {answers_folder}")

    analysis_folder = base_path / "analysis" / f"{args.base}_vs_{args.compare}"
    analysis_folder.mkdir(parents=True, exist_ok=True)

    setup_logger(analysis_folder, args.base, args.compare, args.model)

    logging.info(f"Loading answers from: {answers_folder}")

    # -----------------------------
    # Find JSON answer files
    # -----------------------------
    json_files = sorted(list(answers_folder.glob("*.json")))
    if not json_files:
        raise FileNotFoundError(f"No JSON files found in {answers_folder}")

    logging.info(f"Found {len(json_files)} JSON files: {[f.name for f in json_files]}")

    # -----------------------------
    # Load and merge reports
    # -----------------------------
    df = load_and_merge_reports(json_files)
    logging.info(f"Merged DataFrame with {len(df)} rows")

    # -----------------------------
    # Calculate agreement
    # -----------------------------
    calculate_agreement(df, analysis_folder)

    # -----------------------------
    # Perform further analysis
    # -----------------------------
    agreement_x_analysis(df, analysis_folder)

    logging.info("Analysis complete!")


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        logging.error(f"Error during analysis: {e}", exc_info=True)
        sys.exit(1)


# """
# Purpose: Load, merge, and analyze report datasets generated by different sources/models.

# Inputs:
#     - JSON files of model/report outputs located in ../data

# Outputs:
#     - Merged DataFrame
#     - Agreement metrics printed to console
#     - Optional further analysis per task
# """


# import sys
# import os
# from pathlib import Path

# # -----------------------------
# # Adjust module path
# # -----------------------------
# module_dir = "../../"  # relative path to your src folder
# sys.path.append(os.path.abspath(module_dir))

# # -----------------------------
# # Import functions
# # -----------------------------
# from src.eval.prompting import *
# from src.eval.evaluation import *

# # -----------------------------
# # Parameters
# # -----------------------------
# # Dataset names (rs = report source)
# #rs_list = ["rsllm(gpt-4o)", "rsgroundtruth"]         # Can add more report sources
# rs_list = ["rsdiscovera(gpt-4o)", "rsgroundtruth"]         # Can add more report sources

# ans_prefix = "20"                # Can be modified for different answer sets
# provider = "openai"              # Provider used
# model_name = "gpt-5"             # Model used
# data_dir = Path("/Users/danielapintoveizaga/Documents/discovera/data/answers/compare_groundtruth_vs_discovera/")       # Base data path

# #data_dir = Path("/Users/danielapintoveizaga/Documents/discovera/data/answers/compare_groundtruth_vs_llm/")       # Base data path


# # -----------------------------
# # Construct file paths dynamically
# # -----------------------------
# file_paths = [
#     data_dir / f"ans{ans_prefix}_{rs}_openai_{model_name}.json"
#     for rs in rs_list
# ]
# print(file_paths)
# # -----------------------------
# # Load and merge reports
# # -----------------------------
# df = load_and_merge_reports(file_paths)

# # -----------------------------
# # Calculate agreement
# # -----------------------------
# calculate_agreement(df)

# # -----------------------------
# # Perform further analysis (e.g., per-task agreement)
# # -----------------------------
# agreement_x_analysis(df)

# # -----------------------------
# # Optional: print summary
# # -----------------------------
# print("Analysis complete!")
