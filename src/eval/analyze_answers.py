#!/usr/bin/env python3
"""
Purpose: Load, merge, and analyze report datasets generated by different sources/models.

Inputs:
    - JSON files of model/report outputs located in experiment answers/<base>_vs_<compare>/

Outputs:
    - Merged DataFrame
    - Agreement metrics printed to console
    - Further analysis per task
    - Results/logs saved into analysis/<base>_vs_<compare>/
"""

import sys
import os
import argparse
import logging
from pathlib import Path

# -----------------------------
# Adjust module path
# -----------------------------
module_dir = "../../"  # relative path to src folder
sys.path.append(os.path.abspath(module_dir))

# -----------------------------
# Import functions
# -----------------------------

from src.eval.evaluation import calculate_agreement, agreement_x_analysis, load_and_merge_reports, plot_task_heatmaps_by_source


def parse_args():
    parser = argparse.ArgumentParser(
        description="Analyze pairwise answers: merge reports and compute agreement metrics."
    )
    parser.add_argument(
        "--experiment",
        type=str,
        required=True,
        help="Experiment folder name under data/experiments/"
    )
    parser.add_argument(
        "--base",
        type=str,
        required=True,
        help="Base report key (e.g., groundtruth)"
    )
    parser.add_argument(
        "--compare",
        type=str,
        required=True,
        help="Compare report key (e.g., discovera(gpt-4o))"
    )
    parser.add_argument(
        "--model",
        type=str,
        default="gpt-5",
        help="Model name (e.g., gpt-5)"
    )
    parser.add_argument(
        "--provider",
        type=str,
        default="openai",
        help="Provider name (default: openai)"
    )
    return parser.parse_args()


def setup_logger(log_dir: Path, base: str, compare: str, model: str):
    log_dir.mkdir(parents=True, exist_ok=True)
    log_file = log_dir / f"analysis_{base}_vs_{compare}_{model}.log"
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(message)s",
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler(log_file, mode="w")
        ]
    )
    logging.info(f"Logging to {log_file}")


def main():
    args = parse_args()

    # -----------------------------
    # Define paths
    # -----------------------------
    base_path = Path("../../data/experiments") / args.experiment
    answers_folder = base_path / "answers" / f"{args.base}_vs_{args.compare}"
    if not answers_folder.exists():
        raise FileNotFoundError(f"Answers folder not found: {answers_folder}")

    analysis_folder = base_path / "analysis" / f"{args.base}_vs_{args.compare}"
    analysis_folder.mkdir(parents=True, exist_ok=True)

    setup_logger(analysis_folder, args.base, args.compare, args.model)

    logging.info(f"Loading answers from: {answers_folder}")

    # -----------------------------
    # Find JSON answer files
    # -----------------------------
    json_files = sorted(list(answers_folder.glob("*.json")))
    if not json_files:
        raise FileNotFoundError(f"No JSON files found in {answers_folder}")

    logging.info(f"Found {len(json_files)} JSON files: {[f.name for f in json_files]}")

    # -----------------------------
    # Load and merge reports
    # -----------------------------
    df = load_and_merge_reports(json_files)
    logging.info(f"Merged DataFrame with {len(df)} rows")

    # -----------------------------
    # Calculate agreement
    # -----------------------------
    _, _, df_new = calculate_agreement(df, analysis_folder)

    p = plot_task_heatmaps_by_source(df_new)
    p
    # -----------------------------
    # Perform further analysis
    # -----------------------------
    agreement_x_analysis(df, analysis_folder)

    logging.info("Analysis complete!")


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        logging.error(f"Error during analysis: {e}", exc_info=True)
        sys.exit(1)
