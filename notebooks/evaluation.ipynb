{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b081af-e61b-4600-b1aa-f8576274ac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import toml \n",
    "import json\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "\n",
    "from typing import List, Dict, Callable\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def load_openai_key(beaker_conf_path=\"../.beaker.conf\"):\n",
    "    \"\"\"\n",
    "    Load OpenAI API key from a Beaker configuration file.\n",
    "\n",
    "    Args:\n",
    "        beaker_conf_path (str): Path to the .beaker.conf file.\n",
    "\n",
    "    Returns:\n",
    "        str: OpenAI API key.\n",
    "    \"\"\"\n",
    "    config = toml.load(beaker_conf_path)\n",
    "    print(\"Beaker config loaded successfully.\")\n",
    "\n",
    "    openai_api_key = config['providers']['openai']['api_key']\n",
    "    #print(\"Loaded API key (first 10 chars):\", openai_api_key[:10] + \"...\")\n",
    "    \n",
    "    return openai_api_key\n",
    "\n",
    "def prompt_multiple_questions_template(report_text: str, n: int) -> str:\n",
    "    return f\"\"\"\n",
    "    You are a biomedical researcher. Generate {n} multiple-choice questions from this gene set report:\n",
    "    {report_text}\n",
    "    \n",
    "    Format the output as a JSON list of objects with:\n",
    "    - \"question\": string,\n",
    "    - \"choices\": list of 4 strings,\n",
    "    - \"correct\": string (the correct answer)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e261fe2f-5b48-4968-ac71-7b24b9f4894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OpenAILLM:\n",
    "    def __init__(self, client, model=\"gpt-4o-mini\"):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "\n",
    "    def run(self, prompt: str, json_output: bool = True):\n",
    "        \"\"\"\n",
    "        Sends a prompt to the model and returns the response.\n",
    "        \n",
    "        Parameters:\n",
    "        - prompt: str, the instruction or task for the model.\n",
    "        - json_output: bool, if True, parse output as JSON, else return raw text.\n",
    "        \n",
    "        Returns:\n",
    "        - dict or str: Parsed JSON if json_output=True, else raw text.\n",
    "        \"\"\"\n",
    "        resp = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "\n",
    "        content = resp.choices[0].message.content.strip()\n",
    "        if json_output:\n",
    "            try:\n",
    "                return json.loads(content)\n",
    "            except Exception:\n",
    "                content_clean = re.sub(r\"^```json\\s*|\\s*```$\", \"\", content, flags=re.DOTALL).strip()\n",
    "                return json.loads(content_clean)\n",
    "\n",
    "def get_llm(backend: str, **kwargs):\n",
    "\n",
    "    if backend == \"openai\":\n",
    "        from openai import OpenAI\n",
    "        return OpenAILLM(OpenAI(api_key=kwargs.get(\"api_key\")), model=kwargs.get(\"model\", \"gpt-4o-mini\"))\n",
    "    elif backend == \"anthropic\":\n",
    "        # TODO: Add Anthropic\n",
    "        pass\n",
    "    elif backend == \"hf\":\n",
    "        # TODO: Add HuggingFace \n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown LLM backend: {backend}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6dfbeb-255d-491c-88c2-ded8e8aed46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_reports(csv_path: str, report_column: str = \"Ground Truth\", id_column: str = \"ID\", source: str = \"gt\"):\n",
    "    reports = []\n",
    "    with open(csv_path, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for idx, row in enumerate(reader):\n",
    "            report_id = row.get(id_column)\n",
    "            report_text = row.get(report_column)\n",
    "            if report_text and report_text.strip():\n",
    "                report_key = f\"{source}_{idx}\"\n",
    "                reports.append((report_key, report_id, report_text))\n",
    "    return reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f8bb51-1dc9-4832-8dd0-90f41ee4ac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_questions(\n",
    "    reports: List[tuple],\n",
    "    prompt_template: Callable[[str, int], str],\n",
    "    llm,\n",
    "    num_questions: int = 20,\n",
    "    output_path: str = None\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Generate MCQs from pre-loaded reports using a given prompt template and LLM.\n",
    "    \n",
    "    Args:\n",
    "        reports: List of tuples (report_id, report_text).\n",
    "        prompt_template: Function to generate prompt from text and question count.\n",
    "        llm: LLM object with a .run(prompt, json_output=True) method.\n",
    "        num_questions: Number of MCQs per report.\n",
    "        output_path: Optional path to save the result as JSON.\n",
    "\n",
    "    Returns:\n",
    "        List of MCQ dictionaries.\n",
    "    \"\"\"\n",
    "    multiple_choice_questions = []\n",
    "\n",
    "    for report_key, _ , report_text in tqdm(reports):\n",
    "        try:\n",
    "            prompt = prompt_template(report_text, num_questions)\n",
    "            mcqs = llm.run(prompt, json_output=True)\n",
    "\n",
    "            if isinstance(mcqs, list):\n",
    "                for q in mcqs:\n",
    "                    multiple_choice_questions.append({\n",
    "                        \"report_id\": report_key,\n",
    "                        \"question\": q.get(\"question\", \"\"),\n",
    "                        \"choices\": q.get(\"choices\", []),\n",
    "                        \"correct\": q.get(\"correct\", \"\")\n",
    "                    })\n",
    "            else:\n",
    "                print(f\"[Warning] Unexpected output format for report {report_id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Report {report_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if output_path:\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(multiple_choice_questions, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    return multiple_choice_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f155976-5a9c-4ccf-b824-20a5926df322",
   "metadata": {},
   "outputs": [],
   "source": [
    "src=\"gt\"\n",
    "reports = load_reports(\"../data/benchmark.csv\", report_column=\"Ground Truth\", source=src)\n",
    "llm = get_llm(\"openai\", api_key=load_openai_key())\n",
    "\n",
    "mcqs = generate_questions(\n",
    "    reports=reports,\n",
    "    prompt_template=prompt_multiple_questions_template,\n",
    "    llm=llm,\n",
    "    num_questions=20,\n",
    "    output_path=f\"../data/qs_{src}.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5275bb01-f409-46c3-b07a-bc7595b2ff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfd8bbe-f988-4dc7-babf-e0fc267d33fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326c5772-9cd6-4d17-8ba8-4bc8506d092d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gene-natu",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
